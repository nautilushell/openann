<!DOCTYPE html>
<html>
	<head>
		<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
		<script type="text/javascript" async
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">

		</script>
		<style>
			p {
			margin: 20px 20px 20px 20px;
			}
		body {counter-reset: h2}
		h2 {counter-reset: h3}
		h3 {counter-reset: h4}
		h4 {counter-reset: h5}
		h5 {counter-reset: h6}
		h2:before {counter-increment: h2; content: counter(h2) ". "}
		h3:before {counter-increment: h3; content: counter(h2) "." counter(h3) ". "}
		h4:before {counter-increment: h4; content: counter(h2) "." counter(h3) "." counter(h4) ". "}
		h5:before {counter-increment: h5; content: counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) ". "}
		h6:before {counter-increment: h6; content: counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) "." counter(h6) ". "}
		h2.nocount:before, h3.nocount:before, h4.nocount:before, h5.nocount:before, h6.nocount:before { content: ""; counter-increment: none }
		</style>
	<style>
		.math-left-align .mjx-chtml.MJXc-display{
    	text-align: left !important;
    	\hspace{50cm}
 	}
	</style>
	<style type="text/css">
<!--
 .tab { margin-left: 40px; }
-->
</style>
</head>

<body style="margin-left: 50px">

<h1>Neural Network Math</h1>
<h2>Structure </h2>
<center>
<img height=500; src="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs41024-019-0054-8/MediaObjects/41024_2019_54_Fig2_HTML.png"></center>

<h3>Layers</h3>
<ul>Assume:
<li>784 input nodes (a flatten array (list) from a image which is 28x28)</li>
<li>200 hidden nodes</li>
<li>10 output notes (representing 0, 1, 2, 3, 4, 5, 6, 7, 8 9)</li>
</ul>

<h2>Notation</h2>
<ul>
<li>input nodes = \(x_i\:where\:i=1,784\); \(\text{in matrix form: }\boldsymbol{X \text{ which is a 784 x 1 vector}}\)</li>
<li>hidden notes = \(h_j\:where\:j=1,200\); \(\text{in matrix form: }\boldsymbol{H \text{ which is a 200 x 1 vector}}\)</li>
<li>output notes = \(o_k\:where\:k=1,10\); \(\text{in matrix form: }\boldsymbol{O \text{ which is a 10 x 1 vector}}\)</li>
<li>target = \(y\): given a particular input \(X\) where \(X\) represents the entire input vector for one training input. 10 x 1 vector</li>
<li>calculated target = \(\hat{y}\): network calculated value for input \(X\). 10 x 1 vector</li>
<li>learning rate = \(lr\): a small constant (a scale factor) which adjusts the weights on each training pass. scalar.</li>
<li>bias = \(b\): an addition constant weight added to each layer (e.g., \(x_0,\:h_0\)). scalar.  We are going to ignore this for now.
</ul>

<h3>Weights (linkages between layers)</h3>
<ul>
<li>\(W_{ij}\) represents the link between Node i in input layer to Node j next layer</li>
<li><i>wih</i> is the weight matrix between the input and hidden layers. wih = 200 x 784</li>
<li><i>who</i> is the weight matrix between the hidden layer and the output layers.  who = 10 x 200</li>
</ul>

<h2>Activation function -- the <i>Sigmoid function</i></h2>
\(\huge g(x)= \huge\frac{1}{1+e^{-x}} \) This is used to normalize the output of each neuron between 0 and 1.
<img src="https://docs.scipy.org/doc/scipy/reference/_images/scipy-special-expit-1.png">

<h2>Hypothesis: Calculating \(\hat{y}\)</h2>
<ul>
<li>\(\displaystyle h_j = g\big(\sum_{i=1}^{784}w_{ij}*x_i\big)\:or\: H=g(X * wih.T)\) where ".T" means transpose</li>
<li>\(\displaystyle \hat{y}_j = g\big(\sum_{i=1}^{200}w_{ij}*h_i\big)\:or\: \hat{y}=g(H * who.T)\)
<p></p>
<li>Therefore:  \(\hat{y}=g(g(X * wih.T) * who.T)\)  This is also known as <i>forward propagation</i>.
</ul>

<h2>Error Calculation</h2>

<p>There are many available loss functions, and the nature of our problem should dictate our choice of loss function. The usual error function used is a sum-of-sqaures error function.

\[Loss(y,\hat{y})=\sum_{i=1}^{n} (y - \hat{y})^2\]


<p>\((y - \hat{y})\) is a 10 x 1 vector and it is the amount of error associated with each output node</p>
<p>The goal is to minimize this function.  We do this by figuring out which the direction is lower by looking at the <i>gradient</i> or derivative of this function and stepping toward a lower value. This is done by adjusting the weights \(w_{ij}\) by the a small amount (the learning rate \(lr\)) based on how much of the error is associated with this particular weight. This is determined via the partial derviative of the particular weight to the hypothsis equation.</p>

<p class="tab"><img src="https://miro.medium.com/max/700/1*3FgDOt4kJxK2QZlb9T0cpg.png"</li></p>

<p>However, we cannot directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the chain rule to help us calculate it.</p>

\[\begin{align}\frac{\partial Loss(y,\hat{y})}{\partial W} &= \frac{Loss(y,\hat{y})}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial z} \times \frac{\partial z}{\partial W}\:where\:z=Wx + b \\

&= 2(y-\hat{y}) \times derviative\:of\:sigmoid\:function \times x \\

&=2(y-\hat{y}) \times z(1-z) \times x \end{align}\]

<p>This has to be done for each of the two layers. You start with the error between the output layer and the hidden layer adjusting the values (weights) of \(who\). Then between the hidden layer and the input layer adjusting the values (weights) of \(wih\).  This is called <i>back propagation</i>. The calculation above was only for a single layer. Specifically,</p>

\[wih = wih + deltawih.T\]
\[who = who + deltawho.T\]

\[where\]

\[deltawho = lr \times (H.T * (2(y-\hat{y})\hat{y}\prime))\]
\[deltawih = lr \times \large\left[ X.T * (((2(y-\hat{y})\hat{y}\prime) * who) \times H\prime) \right]\]

<p>And to calculate the derviative of the sigmoid function:</p>
\[\begin{align}\frac{d g(x)}{dx}\frac{1}{1+e^{-x}} &=\frac{e^{-x}}{(1+e^{-x})^2}\\

&=\frac{e^{-x} +1 -1}{(1+e^{-x})^2} \\

&=\frac{e^{-x} +1}{(1+e^{-x})^2} - \frac{1}{(1+e^{-x})^2} \\

&=\frac{1}{(1+e^{-x})} - \frac{1}{(1+e^{-x})^2} \\

&=g(x)-g(x)^2 \\

&=g(x)(1-g(x))

\end{align}\]


<p>Watch this video for more background on the calculus:</p>
<center>
<p><iframe width="420" height="315"
src="https://www.youtube.com/embed/tIeHLnjs5U8">
</iframe></p></center>



<h2>Training the network</h2>
<code>
<p>For each record in the training set X and each expected value (target) y:</p>
<p>Do while training examples {</p>
<p class="tab">forwardProp(X)</p>
<p class="tab">backProp(X,y)</p>
<p>}</p>
</code>


<h2>Predicting once the network is trained</h2>
<p>Simply (a) read in 28x28 picture and put into scalar form (784 x 1).  (b) Then call the function \(\hat{y}=g(g(X * wih.T) * who.T)\) where \(wih\) and \(who\) are the saved values from the training.  The function would be something like:</p>

		<p>
			def guess(self, X):
			<p class="tab">\(\hat{y}=g(g(X * wih.T) * who.T)\)</p>
			<p class="tab">return \(\hat{y}\) # which is a 10 x 1 array with a single 1 in an array element. The index to that 1 is the number predicted.</p>
		</p>



	</body>
</html>